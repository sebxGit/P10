{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informer Implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture has three distinctive features:\n",
    "* A ProbSparse self-attention mechanism with an O time and memory complexity Llog(L).\n",
    "* A self-attention distilling process that prioritizes attention and efficiently handles long input sequences.\n",
    "* An MLP multi-step decoder that predicts long time-series sequences in a single forward operation rather than step-by-step.\n",
    "\n",
    "The Informer model utilizes a three-component approach to define its embedding:\n",
    "* It employs encoded autoregressive features obtained from a convolution network.\n",
    "* 'It uses window-relative positional embeddings derived from harmonic functions.\n",
    "* Absolute positional embeddings obtained from calendar features are utilized.\n",
    "\n",
    "Inspiration from: \n",
    "* https://github.com/zhouhaoyi/Informer2020/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "pl.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code from Informer2020 github (Informer implemented) (Paper implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                             (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, L, d_model)\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "\n",
    "class ProbAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A complete version of the probabilistic attention module.\n",
    "    For simplicity, this implementation computes full attention,\n",
    "    but it is structured similarly to the Informer2020 version.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n",
    "        super(ProbAttention, self).__init__()\n",
    "        self.mask_flag = mask_flag\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask=None):\n",
    "        # queries, keys, values: (B, L, H, D)\n",
    "        # Permute to (B, H, L, D)\n",
    "        queries = queries.permute(0, 2, 1, 3)\n",
    "        keys = keys.permute(0, 2, 1, 3)\n",
    "        values = values.permute(0, 2, 1, 3)\n",
    "\n",
    "        # Compute full attention scores\n",
    "        scores = torch.matmul(\n",
    "            queries, keys.transpose(-2, -1))  # (B, H, Lq, Lk)\n",
    "        if self.scale:\n",
    "            scores = scores / self.scale\n",
    "        # Apply mask only for self-attention (query and key lengths match)\n",
    "        if self.mask_flag and attn_mask is None and (queries.size(-2) == keys.size(-2)):\n",
    "            L = queries.size(-2)\n",
    "            attn_mask = torch.tril(torch.ones(\n",
    "                L, L, device=queries.device)).unsqueeze(0).unsqueeze(0)\n",
    "            scores = scores.masked_fill(attn_mask == 0, -1e9)\n",
    "        elif attn_mask is not None:\n",
    "            scores = scores.masked_fill(attn_mask == 0, -1e9)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.matmul(attn, values)  # (B, H, Lq, D)\n",
    "        output = output.permute(0, 2, 1, 3)    # (B, Lq, H, D)\n",
    "        if self.output_attention:\n",
    "            return output, attn\n",
    "        else:\n",
    "            return output, None\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        self.d_head = d_model // n_heads\n",
    "        self.n_heads = n_heads\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "        # Use the complete ProbAttention module\n",
    "        self.attention = ProbAttention(\n",
    "            scale=self.d_head ** 0.5, attention_dropout=dropout)\n",
    "\n",
    "    def forward(self, q, k, v, attn_mask=None):\n",
    "        B, Lq, _ = q.shape\n",
    "        B, Lk, _ = k.shape  # Use k's own sequence length\n",
    "        q = self.q_linear(q).view(B, Lq, self.n_heads, self.d_head)\n",
    "        k = self.k_linear(k).view(B, Lk, self.n_heads, self.d_head)\n",
    "        v = self.v_linear(v).view(B, Lk, self.n_heads, self.d_head)\n",
    "        # Apply attention\n",
    "        out, attn = self.attention(q, k, v, attn_mask)\n",
    "        # Concatenate heads and pass through the final linear layer\n",
    "        out = out.contiguous().view(B, Lq, self.n_heads * self.d_head)\n",
    "        out = self.out_linear(out)\n",
    "        return out\n",
    "# --- Feed Forward Network ---\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "# --- Encoder Layer ---\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        new_x = self.attention(x, x, x, attn_mask)\n",
    "        x = x + self.dropout(new_x)\n",
    "        x = self.norm1(x)\n",
    "        new_x = self.ff(x)\n",
    "        x = x + self.dropout(new_x)\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "# --- Decoder Layer ---\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.cross_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, self_mask=None, cross_mask=None):\n",
    "        new_x = self.self_attention(x, x, x, self_mask)\n",
    "        x = x + self.dropout(new_x)\n",
    "        x = self.norm1(x)\n",
    "        new_x = self.cross_attention(x, enc_out, enc_out, cross_mask)\n",
    "        x = x + self.dropout(new_x)\n",
    "        x = self.norm2(x)\n",
    "        new_x = self.ff(x)\n",
    "        x = x + self.dropout(new_x)\n",
    "        x = self.norm3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- Encoder ---\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, num_layers, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([layer for _ in range(num_layers)])\n",
    "        self.norm = norm_layer if norm_layer is not None else nn.Identity()\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attn_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "# --- Decoder ---\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, num_layers, norm_layer=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([layer for _ in range(num_layers)])\n",
    "        self.norm = norm_layer if norm_layer is not None else nn.Identity()\n",
    "\n",
    "    def forward(self, x, enc_out, self_mask=None, cross_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, self_mask, cross_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "# --- Informer Model ---\n",
    "class Informer(nn.Module):\n",
    "    def __init__(self, enc_in, dec_in, c_out, seq_len, label_len, out_len,\n",
    "                 d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=512, dropout=0.05):\n",
    "        super(Informer, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.label_len = label_len\n",
    "        self.out_len = out_len\n",
    "\n",
    "        # Input embeddings\n",
    "        self.enc_embedding = nn.Linear(enc_in, d_model)\n",
    "        self.dec_embedding = nn.Linear(dec_in, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layer = EncoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "        self.encoder = Encoder(encoder_layer, e_layers,\n",
    "                               norm_layer=nn.LayerNorm(d_model))\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layer = DecoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(decoder_layer, d_layers,\n",
    "                               norm_layer=nn.LayerNorm(d_model))\n",
    "\n",
    "        # Final projection layer\n",
    "        self.projection = nn.Linear(d_model, c_out)\n",
    "\n",
    "    def forward(self, x_enc, x_dec=None):\n",
    "        # x_enc: (B, seq_len, enc_in)\n",
    "        # x_dec: (B, label_len + out_len, dec_in)\n",
    "        if x_dec is None:\n",
    "            zeros = torch.zeros(x_enc.size(0), self.out_len,\n",
    "                                x_enc.size(-1), device=x_enc.device)\n",
    "            x_dec = torch.cat([x_enc[:, -self.label_len:], zeros], dim=1)\n",
    "\n",
    "        enc = self.positional_encoding(self.enc_embedding(x_enc))\n",
    "        dec = self.positional_encoding(self.dec_embedding(x_dec))\n",
    "\n",
    "        enc_out = self.encoder(enc)\n",
    "        dec_out = self.decoder(dec, enc_out)\n",
    "\n",
    "        out = self.projection(dec_out)\n",
    "        return out[:, -self.out_len:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File containing your data (adjust path if needed)\n",
    "DATA_FILE = \"ConsumptionIndustry.csv\"\n",
    "\n",
    "# Read the CSV file.\n",
    "# Adjust delimiter and decimal separator as needed.\n",
    "df = pd.read_csv(DATA_FILE, sep=\";\", decimal=\",\",\n",
    "                 parse_dates=[\"HourUTC\", \"HourDK\"])\n",
    "df = df.sort_values(by=\"HourUTC\")\n",
    "\n",
    "# Normalize the consumption values.\n",
    "scaler = MinMaxScaler()\n",
    "df[\"ConsumptionkWh\"] = scaler.fit_transform(df[[\"ConsumptionkWh\"]])\n",
    "\n",
    "\n",
    "# Create sliding window sequences.\n",
    "def create_sequences(data, input_len=336, output_len=24):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - input_len - output_len):\n",
    "        X.append(data[i: i + input_len])\n",
    "        y.append(data[i + input_len: i + input_len + output_len])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "input_len = 336   # 14 days of hourly data\n",
    "output_len = 24   # next 24 hours\n",
    "X, y = create_sequences(df[\"ConsumptionkWh\"].values, input_len, output_len)\n",
    "\n",
    "# Convert arrays to tensors.\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Create a custom Dataset.\n",
    "class EnergyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "dataset = EnergyDataset(X_tensor, y_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_data, test_data = torch.utils.data.random_split(\n",
    "    dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Lightning Module with Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformerModel(pl.LightningModule):\n",
    "    def __init__(self, input_len=336, output_len=24, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.model = Informer(\n",
    "            enc_in=1, # Number of features in the encoder input\n",
    "            dec_in=1, # Number of features in the decoder input\n",
    "            c_out=1, # Number of output features\n",
    "            seq_len=input_len,\n",
    "            label_len=input_len // 2,\n",
    "            out_len=output_len,\n",
    "            d_model=512,\n",
    "            n_heads=8,\n",
    "            e_layers=2,\n",
    "            d_layers=1,\n",
    "            d_ff=512,\n",
    "            dropout=0.05\n",
    "        )\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, seq_len) -> add feature dimension.\n",
    "        x = x.unsqueeze(-1)\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.criterion(y_pred.squeeze(), y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.criterion(y_pred.squeeze(), y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation & Plotting Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(x)\n",
    "            preds.append(output.squeeze().cpu().numpy())\n",
    "            trues.append(y.cpu().numpy())\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    trues = np.concatenate(trues, axis=0)\n",
    "\n",
    "    rmse = np.sqrt(np.mean((preds - trues) ** 2))\n",
    "    mae = np.mean(np.abs(preds - trues))\n",
    "    mape = np.mean(np.abs((preds - trues) / (trues + 1e-5))) * 100\n",
    "    return rmse, mae, mape, preds, trues\n",
    "\n",
    "\n",
    "def plot_predictions(preds, trues, sample=0, title=\"Prediction vs Actual\", filename=None):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(trues[sample], label=\"Actual\", marker=\"o\")\n",
    "    plt.plot(preds[sample], label=\"Predicted\", marker=\"x\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Normalized Consumption\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    if filename:\n",
    "        plt.savefig(filename)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_comparison(preds_fp32, preds_fp16, trues, sample=0, title=\"FP32 vs FP16 vs Actual\", filename=None):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(trues[sample], label=\"Actual\", marker=\"o\")\n",
    "    plt.plot(preds_fp32[sample], label=\"FP32 Predicted\", marker=\"x\")\n",
    "    plt.plot(preds_fp16[sample], label=\"FP16 Predicted\", marker=\"s\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Normalized Consumption\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    if filename:\n",
    "        plt.savefig(filename)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_memory_usage(fp32_mem, fp16_mem, filename=None):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(['FP32', 'FP16'], [fp32_mem, fp16_mem], color=['blue', 'green'])\n",
    "    plt.ylabel('Peak GPU Memory Usage (MB)')\n",
    "    plt.title('GPU Memory Usage Comparison')\n",
    "    if filename:\n",
    "        plt.savefig(filename)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runner functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_informer(precision_mode=\"32\", epochs=10):\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=epochs,\n",
    "        precision=precision_mode,  # \"32\" for full precision or \"16-mixed\" for mixed precision\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        #devices=1,\n",
    "        log_every_n_steps=10,\n",
    "        max_steps=100\n",
    "    )\n",
    "    model = InformerModel()\n",
    "    trainer.fit(model, train_dataloaders=train_loader,\n",
    "                val_dataloaders=test_loader)\n",
    "    return model\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def run_experiment(precision_mode, epochs=10):\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "    start_time = time.time()\n",
    "    model = train_informer(precision_mode=precision_mode, epochs=epochs)\n",
    "    training_time = time.time() - start_time\n",
    "    peak_memory = (torch.cuda.max_memory_allocated(device) /\n",
    "                   (1024 ** 2)) if torch.cuda.is_available() else 0\n",
    "    model.to(device)\n",
    "    metrics = evaluate_model(model, test_loader, device)\n",
    "    return model, training_time, peak_memory, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Precession vs Mixed Precession Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/testing_env/lib/python3.9/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name      | Type     | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | model     | Informer | 4.2 M  | train\n",
      "1 | criterion | MSELoss  | 0      | train\n",
      "-----------------------------------------------\n",
      "4.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 M     Total params\n",
      "16.849    Total estimated model params size (MB)\n",
      "50        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a27dd4a10f451daf7c7e47671beae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/testing_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/testing_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd682ed6ca9f453f9871112746ed6f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/testing_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:512: You passed `Trainer(accelerator='cpu', precision='16-mixed')` but AMP with fp16 is not supported on CPU. Using `precision='bf16-mixed'` instead.\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type     | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | model     | Informer | 4.2 M  | train\n",
      "1 | criterion | MSELoss  | 0      | train\n",
      "-----------------------------------------------\n",
      "4.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 M     Total params\n",
      "16.849    Total estimated model params size (MB)\n",
      "50        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76feb49f2ba944a6a324c004c74fcb72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6749e757c9e742bfab1e82e81e456a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run experiment for FP32\n",
    "model_fp32, fp32_time, fp32_mem, metrics_fp32 = run_experiment(\"32\", epochs=1)\n",
    "rmse_fp32, mae_fp32, mape_fp32, preds_fp32, trues_fp32 = metrics_fp32\n",
    "\n",
    "# Run experiment for FP16\n",
    "model_fp16, fp16_time, fp16_mem, metrics_fp16 = run_experiment(\n",
    "    \"16-mixed\", epochs=1)\n",
    "rmse_fp16, mae_fp16, mape_fp16, preds_fp16, trues_fp16 = metrics_fp16\n",
    "\n",
    "# Print summary metrics\n",
    "print(\"=== FP32 ===\")\n",
    "print(f\"Training Time: {fp32_time:.2f} sec\")\n",
    "print(f\"Peak GPU Memory: {fp32_mem:.2f} MB\")\n",
    "print(f\"RMSE: {rmse_fp32:.4f} | MAE: {mae_fp32:.4f} | MAPE: {mape_fp32:.2f}%\\n\")\n",
    "\n",
    "print(\"=== FP16 ===\")\n",
    "print(f\"Training Time: {fp16_time:.2f} sec\")\n",
    "print(f\"Peak GPU Memory: {fp16_mem:.2f} MB\")\n",
    "print(f\"RMSE: {rmse_fp16:.4f} | MAE: {mae_fp16:.4f} | MAPE: {mape_fp16:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(preds_fp32, trues_fp32, sample=0,\n",
    "                 title=\"FP32: Prediction vs Actual\", filename=\"fp32_prediction.png\")\n",
    "plot_predictions(preds_fp16, trues_fp16, sample=0,\n",
    "                 title=\"FP16: Prediction vs Actual\", filename=\"fp16_prediction.png\")\n",
    "plot_comparison(preds_fp32, preds_fp16, trues_fp32, sample=0,\n",
    "                title=\"Comparison: FP32 vs FP16 vs Actual\", filename=\"combined_comparison.png\")\n",
    "plot_memory_usage(fp32_mem, fp16_mem, filename=\"gpu_memory_usage.png\")\n",
    "\n",
    "summary_text = (\n",
    "    \"=== Summary of Results ===\\n\"\n",
    "    f\"FP32  --> Training Time: {fp32_time:.2f} sec | Peak GPU Memory: {fp32_mem:.2f} MB | RMSE: {rmse_fp32:.4f}, MAE: {mae_fp32:.4f}, MAPE: {mape_fp32:.2f}%\\n\"\n",
    "    f\"FP16  --> Training Time: {fp16_time:.2f} sec | Peak GPU Memory: {fp16_mem:.2f} MB | RMSE: {rmse_fp16:.4f}, MAE: {mae_fp16:.4f}, MAPE: {mape_fp16:.2f}%\\n\"\n",
    ")\n",
    "print(summary_text)\n",
    "with open(\"results_summary.txt\", \"w\") as f:\n",
    "    f.write(summary_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
