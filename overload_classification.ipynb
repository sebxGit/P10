{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Training AdaBoostRegressor model-----\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from argparse import ArgumentParser\n",
    "import holidays\n",
    "import optuna\n",
    "from models.D_PAD_adpGCN import DPAD_GCN\n",
    "from models.LSTM import LSTM\n",
    "from models.GRU import GRU\n",
    "from models.MLP import MLP\n",
    "from models.xPatch import xPatch\n",
    "from models.PatchMixer import PatchMixer\n",
    "from models.Fredformer import Fredformer\n",
    "import argparse\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.utils import resample\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import ast\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import BasePredictionWriter\n",
    "from lightning.pytorch import seed_everything\n",
    "\n",
    "# Seed \n",
    "SEED = 42\n",
    "seed_everything(SEED, workers=True)\n",
    "\n",
    "def convert_Colorado_to_hourly(data):\n",
    "\n",
    "    # Remove unnecessary columns\n",
    "    data = data.drop(columns=['Zip_Postal_Code'])\n",
    "\n",
    "    # Convert date/time columns to datetime\n",
    "    data['Start_DateTime'] = pd.to_datetime(data['Start_DateTime'])\n",
    "    data['Charging_EndTime'] = pd.to_datetime(data['End_DateTime'])\n",
    "    data['Charging_Time'] = pd.to_timedelta(data['Charging_Time'])\n",
    "\n",
    "    ####################### CONVERT DATASET TO HOURLY  #######################\n",
    "\n",
    "    # Split the session into hourly intervals\n",
    "    hourly_rows = []\n",
    "\n",
    "    # Iterate over each row in the dataframe to break charging sessions into hourly intervals\n",
    "    for _, row in data.iterrows():\n",
    "        start, end = row['Start_DateTime'], row['Charging_EndTime']\n",
    "        energy = row['Energy_Consumption']\n",
    "\n",
    "        # Generate hourly intervals\n",
    "        hourly_intervals = pd.date_range(\n",
    "            start=start.floor('h'), end=end.ceil('h'), freq='h')\n",
    "        total_duration = (end - start).total_seconds()\n",
    "\n",
    "        for i in range(len(hourly_intervals) - 1):\n",
    "            interval_start = max(start, hourly_intervals[i])\n",
    "            interval_end = min(end, hourly_intervals[i+1])\n",
    "            interval_duration = (interval_end - interval_start).total_seconds()\n",
    "\n",
    "            # Calculate the energy consumption for the interval if interval is greater than 0 (Start and end time are different)\n",
    "            if interval_duration > 0:\n",
    "                energy_fraction = (interval_duration / total_duration) * energy\n",
    "\n",
    "            hourly_rows.append({\n",
    "                'Time': hourly_intervals[i],\n",
    "                'Energy_Consumption': energy_fraction,\n",
    "                \"Session_Count\": 1  # Count of sessions in the interval\n",
    "            })\n",
    "\n",
    "    # Create a new dataframe from the hourly intervals\n",
    "    hourly_df = pd.DataFrame(hourly_rows)\n",
    "\n",
    "    # Aggregate the hourly intervals\n",
    "    hourly_df = hourly_df.groupby('Time').agg({\n",
    "        'Energy_Consumption': 'sum',\n",
    "        'Session_Count': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Convert the Time column to datetime\n",
    "    hourly_df['Time'] = pd.to_datetime(\n",
    "        hourly_df['Time'], format=\"%d-%m-%Y %H:%M:%S\")\n",
    "    hourly_df = hourly_df.set_index('Time')\n",
    "\n",
    "    # Define time range for all 24 hours\n",
    "    start_time = hourly_df.index.min().normalize()  # 00:00:00\n",
    "    end_time = hourly_df.index.max().normalize() + pd.Timedelta(days=1) - \\\n",
    "        pd.Timedelta(hours=1)  # 23:00:00\n",
    "\n",
    "    # Change range to time_range_full, so from 00:00:00 to 23:00:00\n",
    "    time_range_full = pd.date_range(start=start_time, end=end_time, freq='h')\n",
    "\n",
    "    # Reindex the hourly data to include all hours in the range\n",
    "    hourly_df = hourly_df.reindex(time_range_full, fill_value=0)\n",
    "\n",
    "    # Return the hourly data\n",
    "    return hourly_df\n",
    "\n",
    "def convert_SDU_to_hourly(df):\n",
    "  df = df.set_index('Timestamp')\n",
    "\n",
    "  hourly = df.resample('h').agg({\n",
    "      'Total number of EVs':      'sum',\n",
    "      'Number of charging EVs':   'sum',\n",
    "      'Number of driving EVs':    'sum',\n",
    "      'Total grid load':          'sum',\n",
    "      'Aggregated base load':     'sum',\n",
    "      'Aggregated charging load': 'sum',\n",
    "      'Overload duration [min]':  'sum',\n",
    "  })\n",
    "\n",
    "  return hourly\n",
    "\n",
    "def add_features(hourly_df, dataset_name, historical_feature, weather_df=None):\n",
    "  ####################### TIMED BASED FEATURES  #######################\n",
    "  hourly_df['Day_of_Week'] = hourly_df.index.dayofweek\n",
    "\n",
    "  # Add hour of the day\n",
    "  hourly_df['Hour_of_Day'] = hourly_df.index.hour\n",
    "\n",
    "  # Add month of the year\n",
    "  hourly_df['Month_of_Year'] = hourly_df.index.month\n",
    "\n",
    "  # Add year\n",
    "  hourly_df['Year'] = hourly_df.index.year\n",
    "\n",
    "  # Add day/night\n",
    "  hourly_df['Day/Night'] = (hourly_df['Hour_of_Day'] >= 6) & (hourly_df['Hour_of_Day'] <= 18)\n",
    "\n",
    "  # Add holiday\n",
    "  if dataset_name == 'Colorado':\n",
    "    us_holidays = holidays.US(years=range(hourly_df.index.year.min(), hourly_df.index.year.max() + 1))\n",
    "    hourly_df['IsHoliday'] = hourly_df.index.to_series().dt.date.isin(us_holidays).astype(int)\n",
    "  elif dataset_name == 'SDU':\n",
    "    dk_holidays = holidays.DK(years=range(\n",
    "        hourly_df.index.year.min(), hourly_df.index.year.max() + 1))\n",
    "    hourly_df['IsHoliday'] = hourly_df.index.to_series().dt.date.isin(dk_holidays).astype(int)\n",
    "\n",
    "  # Add weekend\n",
    "  hourly_df['Weekend'] = (hourly_df['Day_of_Week'] >= 5).astype(int)\n",
    "\n",
    "  ####################### CYCLIC FEATURES  #######################\n",
    "  # Cos and sin transformations for cyclic features (hour of the day, day of the week, month of the year)\n",
    "\n",
    "  hourly_df['HourSin'] = np.sin(2 * np.pi * hourly_df['Hour_of_Day'] / 24)\n",
    "  hourly_df['HourCos'] = np.cos(2 * np.pi * hourly_df['Hour_of_Day'] / 24)\n",
    "  hourly_df['DayOfWeekSin'] = np.sin(2 * np.pi * hourly_df['Day_of_Week'] / 7)\n",
    "  hourly_df['DayOfWeekCos'] = np.cos(2 * np.pi * hourly_df['Day_of_Week'] / 7)\n",
    "  hourly_df['MonthOfYearSin'] = np.sin(2 * np.pi * hourly_df['Month_of_Year'] / 12)\n",
    "  hourly_df['MonthOfYearCos'] = np.cos(2 * np.pi * hourly_df['Month_of_Year'] / 12)\n",
    "\n",
    "  ####################### SEASONAL FEATURES  #######################\n",
    "  # 0 = Spring, 1 = Summer, 2 = Autumn, 3 = Winter\n",
    "  month_to_season = {1: 4, 2: 4, 3: 0, 4: 0, 5: 0, 6: 1,\n",
    "                     7: 1, 8: 1, 9: 2, 10: 2, 11: 2, 12: 3}\n",
    "  hourly_df['Season'] = hourly_df['Month_of_Year'].map(month_to_season)\n",
    "\n",
    "  ####################### WEATHER FEATURES  #######################\n",
    "  if weather_df is not None:\n",
    "    weather_df = pd.read_csv(weather_df, parse_dates=['time']).set_index(\n",
    "        'time').rename(columns={'temperature': 'Temperature'})\n",
    "\n",
    "    # make sure tempture is a number\n",
    "    weather_df['Temperature'] = pd.to_numeric(\n",
    "        weather_df['Temperature'], errors='coerce')\n",
    "\n",
    "    hourly_df = hourly_df.join(weather_df, how='left')\n",
    "\n",
    "  ####################### HISTORICAL CONSUMPTION FEATURES  #######################\n",
    "  # Lag features\n",
    "  # 1h\n",
    "  hourly_df['Energy_Consumption_1h'] = hourly_df[historical_feature].shift(1)\n",
    "\n",
    "  # 6h\n",
    "  hourly_df['Energy_Consumption_6h'] = hourly_df[historical_feature].shift(6)\n",
    "\n",
    "  # 12h\n",
    "  hourly_df['Energy_Consumption_12h'] = hourly_df[historical_feature].shift(\n",
    "      12)\n",
    "\n",
    "  # 24h\n",
    "  hourly_df['Energy_Consumption_24h'] = hourly_df[historical_feature].shift(\n",
    "      24)\n",
    "\n",
    "  # 1 week\n",
    "  hourly_df['Energy_Consumption_1w'] = hourly_df[historical_feature].shift(\n",
    "      24*7)\n",
    "\n",
    "  # Rolling average\n",
    "  # 24h\n",
    "  hourly_df['Energy_Consumption_rolling'] = hourly_df[historical_feature].rolling(window=24).mean()\n",
    "\n",
    "  return hourly_df\n",
    "\n",
    "def filter_data(start_date, end_date, data):\n",
    "    ####################### FILTER DATASET  #######################\n",
    "    data = data[(data.index >= start_date) & (data.index <= end_date)].copy()\n",
    "    # print(data.head())\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "  def __init__(self, X: np.ndarray, y: np.ndarray, seq_len: int = 1, pred_len: int = 24, stride: int = 24):\n",
    "    self.seq_len = seq_len\n",
    "    self.pred_len = pred_len\n",
    "    self.stride = stride\n",
    "\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = y.to_numpy()\n",
    "\n",
    "    # Ensure data is numeric and handle non-numeric values\n",
    "    X = np.asarray(X, dtype=np.float32)\n",
    "    y = np.asarray(y, dtype=np.float32)\n",
    "\n",
    "    self.X = torch.tensor(X).float()\n",
    "    self.y = torch.tensor(y).float()\n",
    "\n",
    "  def __len__(self):\n",
    "    return (len(self.X) - (self.seq_len + self.pred_len - 1)) // self.stride + 1\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    start_idx = index * self.stride\n",
    "    x_window = self.X[start_idx: start_idx + self.seq_len]\n",
    "    y_target = self.y[start_idx + self.seq_len: start_idx + self.seq_len + self.pred_len]\n",
    "    return x_window, y_target\n",
    "\n",
    "class BootstrapSampler:\n",
    "    def __init__(self, dataset_size, random_state=None):\n",
    "        self.dataset_size = dataset_size\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = resample(range(self.dataset_size), replace=True,\n",
    "                           n_samples=self.dataset_size, random_state=self.random_state)\n",
    "        return iter(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "def process_window(i, X, y, seq_len, pred_len):\n",
    "  X_win = X[i:i + seq_len]\n",
    "  y_tar = y[i + seq_len:i + seq_len + pred_len]\n",
    "  arr_x = np.asanyarray(X_win).reshape(-1)\n",
    "  arr_y = np.asanyarray(y_tar).reshape(-1)\n",
    "  return arr_x, arr_y\n",
    "\n",
    "class ColoradoDataModule(L.LightningDataModule):\n",
    "  def __init__(self, data_dir: str, scaler: int, seq_len: int, pred_len: int, stride: int, batch_size: int, num_workers: int, is_persistent: bool):\n",
    "    super().__init__()\n",
    "    self.data_dir = data_dir\n",
    "    self.scaler = scaler\n",
    "    self.seq_len = seq_len\n",
    "    self.pred_len = pred_len\n",
    "    self.stride = stride\n",
    "    self.batch_size = batch_size\n",
    "    self.num_workers = num_workers\n",
    "    self.is_persistent = is_persistent\n",
    "    self.X_train = None\n",
    "    self.y_train = None\n",
    "    self.X_val = None\n",
    "    self.y_val = None\n",
    "    self.X_test = None\n",
    "    self.y_test = None\n",
    "    self.val_dates = []\n",
    "\n",
    "  def setup(self, stage: str):\n",
    "    start_date = pd.to_datetime('2021-05-30')\n",
    "    end_date = pd.to_datetime('2023-05-30')\n",
    "\n",
    "    # Load and preprocess the data\n",
    "    data = pd.read_csv(self.data_dir)\n",
    "    data = convert_Colorado_to_hourly(data)\n",
    "    data = add_features(data, dataset_name='Colorado', historical_feature='Energy_Consumption', weather_df='Colorado/denver_weather.csv')\n",
    "    df = filter_data(start_date, end_date, data)\n",
    "\n",
    "    df = df.dropna()\n",
    "\n",
    "    X = df.copy()\n",
    "\n",
    "    y = X.pop('Energy_Consumption')\n",
    "\n",
    "    # 60/20/20 split\n",
    "    X_tv, self.X_test, y_tv, self.y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(X_tv, y_tv, test_size=0.25, shuffle=False)\n",
    "\n",
    "    self.val_dates = self.X_val.index.tolist()  # Store validation dates for later use\n",
    "\n",
    "    preprocessing = self.scaler\n",
    "    preprocessing.fit(self.X_train)  # should only fit to training data\n",
    "    \n",
    "    if stage == \"fit\" or stage is None:\n",
    "      self.X_train = preprocessing.transform(self.X_train)\n",
    "      self.y_train = np.array(self.y_train)\n",
    "\n",
    "    if stage == \"test\" or \"predict\" or stage is None:\n",
    "      self.X_test = preprocessing.transform(self.X_test)\n",
    "      self.y_test = np.array(self.y_test)\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    train_dataset = TimeSeriesDataset(self.X_train, self.y_train, seq_len=self.seq_len, pred_len=self.pred_len, stride=self.stride)\n",
    "    if args.individual == \"True\":\n",
    "      train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, persistent_workers=self.is_persistent, drop_last=False)\n",
    "    else:\n",
    "      sampler = BootstrapSampler(len(train_dataset), random_state=SEED)\n",
    "      train_loader = DataLoader(train_dataset, batch_size=self.batch_size, sampler=sampler, shuffle=False, num_workers=self.num_workers, persistent_workers=self.is_persistent)\n",
    "    return train_loader\n",
    "  \n",
    "  def predict_dataloader(self):\n",
    "    test_dataset = TimeSeriesDataset(self.X_test, self.y_test, seq_len=self.seq_len, pred_len=self.pred_len, stride=self.stride)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, persistent_workers=self.is_persistent, drop_last=False)\n",
    "    return test_loader\n",
    "  \n",
    "  def sklearn_setup(self, set_name: str = \"train\"):\n",
    "    if set_name == \"train\":\n",
    "        if args.individual == \"True\":\n",
    "          X, y = self.X_train, self.y_train\n",
    "        else:\n",
    "          X, y = resample(self.X_train, self.y_train, replace=True, n_samples=len(self.X_train), random_state=SEED)\n",
    "    elif set_name == \"val\":\n",
    "        X, y = self.X_val, self.y_val\n",
    "    elif set_name == \"test\":\n",
    "        X, y = self.X_test, self.y_test\n",
    "    else:\n",
    "        raise ValueError(\"Invalid set name. Choose from 'train', 'val', or 'test'.\")\n",
    "\n",
    "    seq_len, pred_len, stride = self.seq_len, self.pred_len, self.stride\n",
    "    max_start = len(X) - (seq_len + pred_len) + 1\n",
    "\n",
    "    # Parallelize the loop\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(process_window)(i, X, y, seq_len, pred_len) for i in range(0, max_start, stride)\n",
    "    )\n",
    "\n",
    "    # Unpack results\n",
    "    X_window, y_target = zip(*results)\n",
    "    return np.array(X_window), np.array(y_target)\n",
    "    \n",
    "class SDUDataModule(L.LightningDataModule):\n",
    "  def __init__(self, data_dir: str, scaler: int, seq_len: int, pred_len: int, stride: int, batch_size: int, num_workers: int, is_persistent: bool):\n",
    "    super().__init__()\n",
    "    self.data_dir = data_dir\n",
    "    self.scaler = scaler\n",
    "    self.seq_len = seq_len\n",
    "    self.pred_len = pred_len\n",
    "    self.stride = stride\n",
    "    self.batch_size = batch_size\n",
    "    self.num_workers = num_workers\n",
    "    self.is_persistent = is_persistent\n",
    "    self.X_train = None\n",
    "    self.y_train = None\n",
    "    self.X_val = None\n",
    "    self.y_val = None\n",
    "    self.X_test = None\n",
    "    self.y_test = None\n",
    "\n",
    "  def setup(self, stage: str):\n",
    "    start_date = pd.to_datetime('2024-12-31')\n",
    "    end_date = pd.to_datetime('2032-12-31')\n",
    "\n",
    "    # Load the data\n",
    "    # df = pd.read_csv(self.data_dir, parse_dates=['Timestamp'])\n",
    "    df = pd.read_csv(self.data_dir, skipinitialspace=True)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df['Timestamp'] = df['Timestamp'].str.strip()  # <-- Add this line\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'], format=\"%b %d, %Y, %I:%M:%S %p\")\n",
    "    df = convert_SDU_to_hourly(df)\n",
    "    feature_df = add_features(hourly_df=df, dataset_name='SDU', historical_feature='Aggregated charging load')\n",
    "    df = filter_data(start_date, end_date, feature_df)\n",
    "\n",
    "    df = df.dropna()\n",
    "    X = df.copy()\n",
    "\n",
    "    y = X.pop('Aggregated charging load')\n",
    "\n",
    "    # 60/20/20 split\n",
    "    X_tv, self.X_test, y_tv, self.y_test = train_test_split( X, y, test_size=0.2, shuffle=False)\n",
    "    self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(X_tv, y_tv, test_size=0.25, shuffle=False)\n",
    "\n",
    "    preprocessing = self.scaler\n",
    "    preprocessing.fit(self.X_train)  # should only fit to training data\n",
    "\n",
    "    if stage == \"fit\" or stage is None:\n",
    "      self.X_train = preprocessing.transform(self.X_train)\n",
    "      self.y_train = np.array(self.y_train)\n",
    "\n",
    "    if stage == \"test\" or \"predict\" or stage is None:\n",
    "      self.X_test = preprocessing.transform(self.X_test)\n",
    "      self.y_test = np.array(self.y_test)\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    train_dataset = TimeSeriesDataset(self.X_train, self.y_train, seq_len=self.seq_len, pred_len=self.pred_len, stride=self.stride)\n",
    "    if args.individual == \"True\":\n",
    "      train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, persistent_workers=self.is_persistent, drop_last=False)\n",
    "    else:   \n",
    "      sampler = BootstrapSampler(len(train_dataset), random_state=SEED)\n",
    "      train_loader = DataLoader(train_dataset, batch_size=self.batch_size, sampler=sampler, shuffle=False, num_workers=self.num_workers, persistent_workers=self.is_persistent)\n",
    "    return train_loader\n",
    "\n",
    "  def predict_dataloader(self):\n",
    "    test_dataset = TimeSeriesDataset(self.X_test, self.y_test, seq_len=self.seq_len, pred_len=self.pred_len, stride=self.stride)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, persistent_workers=self.is_persistent, drop_last=False)\n",
    "    return test_loader\n",
    "  \n",
    "  def sklearn_setup(self, set_name: str = \"train\"):\n",
    "    if set_name == \"train\":\n",
    "        if args.individual == \"True\":\n",
    "          X, y = self.X_train, self.y_train\n",
    "        else:\n",
    "          X, y = resample(self.X_train, self.y_train, replace=True, n_samples=len(self.X_train), random_state=SEED)\n",
    "    elif set_name == \"val\":\n",
    "        X, y = self.X_val, self.y_val\n",
    "    elif set_name == \"test\":\n",
    "        X, y = self.X_test, self.y_test\n",
    "    else:\n",
    "        raise ValueError(\"Invalid set name. Choose from 'train', 'val', or 'test'.\")\n",
    "\n",
    "    seq_len, pred_len, stride = self.seq_len, self.pred_len, self.stride\n",
    "    max_start = len(X) - (seq_len + pred_len) + 1\n",
    "\n",
    "    # Parallelize the loop\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(process_window)(i, X, y, seq_len, pred_len) for i in range(0, max_start, stride)\n",
    "    )\n",
    "\n",
    "    # Unpack results\n",
    "    X_window, y_target = zip(*results)\n",
    "    return np.array(X_window), np.array(y_target)\n",
    "\n",
    "class LightningModel(L.LightningModule):\n",
    "  def __init__(self, model, criterion, optimizer, learning_rate):\n",
    "    super().__init__()\n",
    "    self.criterion = criterion\n",
    "    self.learning_rate = learning_rate\n",
    "    self.optimizer = optimizer\n",
    "    self.model = model\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.model(x)\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    y_hat = self(x)\n",
    "    train_loss = self.criterion(y_hat, y) \n",
    "    self.log(\"train_loss\", train_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "    return train_loss\n",
    "\n",
    "  def predict_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    y_hat = self(x)\n",
    "    return y_hat\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    return self.optimizer(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "class Configs:\n",
    "  def __init__(self, config_dict):\n",
    "    for key, value in config_dict.items():\n",
    "      setattr(self, key, value)\n",
    "\n",
    "def plot_and_save_with_metrics(combined_name, colmod):\n",
    "  actuals = []\n",
    "  for batch in colmod.predict_dataloader():\n",
    "    x, y = batch\n",
    "    actuals.extend(y.numpy())\n",
    "\n",
    "  actuals_flat = [item for sublist in actuals for item in sublist]\n",
    "\n",
    "  folder_path = f'Predictions/{combined_name}'\n",
    "  pt_files = [f for f in os.listdir(folder_path) if f.endswith('.pt')]\n",
    "\n",
    "  metrics = []\n",
    "  plt.figure(figsize=(20, 5))\n",
    "  plt.plot(actuals_flat, label='Actuals')\n",
    "  for pt_file in pt_files:\n",
    "    file_path = os.path.join(folder_path, pt_file)\n",
    "    predictions = torch.load(file_path, weights_only=False)\n",
    "    model_name = pt_file.split('_')[1].split('.')[0]\n",
    "    # model_name = pt_file.split('.')[0].split('_')[-1] #use this with loss function names\n",
    "\n",
    "    if type(predictions[0]) == torch.Tensor: \n",
    "      predictions = [elem.item() for tensor in predictions for elem in tensor.flatten()]\n",
    "    elif type(predictions[0]) == np.float64:\n",
    "      predictions = predictions.tolist()\n",
    "\n",
    "    predictions = predictions[-len(actuals_flat):] # reduce length of predictions to match actuals\n",
    "\n",
    "    metrics.append({\n",
    "      'model': model_name,\n",
    "      'mse': mean_squared_error(predictions, actuals_flat),\n",
    "      'mae': mean_absolute_error(predictions, actuals_flat),\n",
    "      'mape': mean_absolute_percentage_error(predictions, actuals_flat)})\n",
    "    plt.plot(predictions, label=model_name)\n",
    "\n",
    "  if metrics:\n",
    "    loss_func_df = pd.concat([pd.DataFrame([m]) for m in metrics], ignore_index=True)\n",
    "  else:\n",
    "    loss_func_df = pd.DataFrame(columns=['model', 'mse', 'mae', 'mape'])\n",
    "  loss_func_df.set_index('model', inplace=True)\n",
    "  loss_func_df.to_csv(f'{folder_path}/loss_func_metrics.csv')\n",
    "\n",
    "  plt.xlabel('Samples')\n",
    "  plt.ylabel('Energy Consumption')\n",
    "  plt.title(f'Predictions vs Actuals ({combined_name})')\n",
    "  plt.legend()\n",
    "\n",
    "  plt.savefig(f'{folder_path}/predictions_vs_actuals_{combined_name}.png')\n",
    "  plt.show()\n",
    "\n",
    "def accuracy_score(TP, TN, FP, FN):\n",
    "  return (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "def precision_score(TP, FP):\n",
    "  return TP / (TP + FP)\n",
    "\n",
    "def recall_score(TP, FN):\n",
    "  return TP / (TP + FN)\n",
    "\n",
    "def initialize_model(model_name, hyperparameters):\n",
    "  model_dict = {\n",
    "  \"LSTM\": lambda: LSTM(input_size=args.input_size, pred_len=args.pred_len, hidden_size=hyperparameters['hidden_size'], num_layers=hyperparameters['num_layers'], dropout=hyperparameters['dropout'] ),\n",
    "  \"GRU\": lambda: GRU(input_size=args.input_size, pred_len=args.pred_len, hidden_size=hyperparameters['hidden_size'], num_layers=hyperparameters['num_layers'], dropout=hyperparameters['dropout']),\n",
    "  \"xPatch\": lambda: xPatch(Configs({**hyperparameters, \"enc_in\": args.input_size, \"pred_len\": args.pred_len, 'seq_len': args.seq_len})),\n",
    "  \"PatchMixer\": lambda: PatchMixer(Configs({**hyperparameters, \"enc_in\": args.input_size, \"pred_len\": args.pred_len, \"seq_len\": args.seq_len})),\n",
    "  \"RandomForestRegressor\": lambda: MultiOutputRegressor(RandomForestRegressor(n_estimators=hyperparameters['n_estimators'], max_depth=hyperparameters['max_depth'], min_samples_split=hyperparameters['min_samples_split'], min_samples_leaf=hyperparameters['min_samples_leaf'], max_features=hyperparameters['max_features'], random_state=SEED), n_jobs=-1),\n",
    "  \"GradientBoostingRegressor\": lambda: MultiOutputRegressor(GradientBoostingRegressor(n_estimators=hyperparameters['n_estimators'], max_depth=hyperparameters['max_depth'], min_samples_split=hyperparameters['min_samples_split'], min_samples_leaf=hyperparameters['min_samples_leaf'], learning_rate=hyperparameters['learning_rate_model'], random_state=SEED), n_jobs=-1),\n",
    "  \"AdaBoostRegressor\": lambda: MultiOutputRegressor(AdaBoostRegressor(n_estimators=hyperparameters['n_estimators'], learning_rate=hyperparameters['learning_rate'], random_state=SEED), n_jobs=-1),\n",
    "  # \"DPAD\": lambda: DPAD_GCN(input_len=args.seq_len, output_len=args.pred_len, input_dim=args.input_size, enc_hidden=dpad_params['enc_hidden'], dec_hidden=dpad_params['dec_hidden'],\n",
    "  #                               dropout=dpad_params['dropout'], num_levels=dpad_params['num_levels'], K_IMP=dpad_params['K_IMP'], RIN=dpad_params['RIN'])\n",
    "  }\n",
    "\n",
    "  return model_dict[model_name]()\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.models = \"AdaBoostRegressor\"  # ['xPatch', 'LSTM', 'GRU', 'PatchMixer']\n",
    "        self.individual = \"True\"\n",
    "        self.input_size = 22\n",
    "        self.pred_len = 24\n",
    "        self.seq_len = 24 * 7\n",
    "        self.stride = 24\n",
    "        self.dataset = \"Colorado\"\n",
    "        self.threshold = 60\n",
    "\n",
    "# Create an instance of the Args class\n",
    "args = Args()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  # support individual model or ensemble\n",
    "  mode = \"ensemble\" if '[' in args.models else \"individual\"\n",
    "  if mode == \"ensemble\":\n",
    "    selected_models = ast.literal_eval(args.models)\n",
    "    combined_name = \"-\".join([m for m in selected_models])\n",
    "  else:\n",
    "    selected_models = [args.models]\n",
    "    combined_name = args.models\n",
    "\n",
    "  predictions = []\n",
    "  metrics = []\n",
    "  for model_name in selected_models:\n",
    "    print(f\"-----Training {model_name} model-----\")\n",
    "\n",
    "    if mode == \"ensemble\":\n",
    "      hparams = pd.read_csv(f'./Tunings/{args.dataset}_{args.pred_len}h_tuning.csv')\n",
    "    else:\n",
    "      hparams = pd.read_csv(f'./Tunings/{args.dataset}_{args.pred_len}h_individual_tuning.csv')\n",
    "\n",
    "    hyperparameters = ast.literal_eval(hparams[hparams['model'] == model_name].iloc[0].values[3])\n",
    "    model = initialize_model(model_name, hyperparameters)\n",
    "\n",
    "    # prepare colmod\n",
    "    if args.dataset == \"Colorado\":\n",
    "      colmod = ColoradoDataModule(data_dir='Colorado/Preprocessing/TestDataset/CleanedColoradoData.csv', scaler=MinMaxScaler(), seq_len=args.seq_len, batch_size=hyperparameters['batch_size'], pred_len=args.pred_len, stride=args.stride, num_workers=hyperparameters['num_workers'], is_persistent=True if hyperparameters['num_workers'] > 0 else False)\n",
    "    else: \n",
    "      colmod = SDUDataModule(data_dir='SDU Dataset/DumbCharging_2020_to_2032/Measurements.csv', scaler=MinMaxScaler(), seq_len=args.seq_len, batch_size=hyperparameters['batch_size'], pred_len=args.pred_len, stride=args.stride, num_workers=hyperparameters['num_workers'], is_persistent=True if hyperparameters['num_workers'] > 0 else False)\n",
    "\n",
    "    colmod.prepare_data()\n",
    "    colmod.setup(stage=None)\n",
    "\n",
    "    # model creates prediction\n",
    "    if isinstance(model, torch.nn.Module):\n",
    "      model = LightningModel(model=model, criterion=nn.L1Loss(), optimizer=torch.optim.Adam, learning_rate=hyperparameters['learning_rate'])\n",
    "      trainer = L.Trainer(max_epochs=hyperparameters['max_epochs'], log_every_n_steps=100, precision='16-mixed', enable_checkpointing=False)\n",
    "      # trainer = L.Trainer(max_epochs=hyperparameters['max_epochs'], log_every_n_steps=100, precision='16-mixed', enable_checkpointing=False, strategy='ddp_find_unused_parameters_true')\n",
    "      trainer.fit(model, colmod)\n",
    "\n",
    "      trainer = L.Trainer(max_epochs=hyperparameters['max_epochs'], log_every_n_steps=100, precision='16-mixed', enable_checkpointing=False, devices=1)\n",
    "      y_pred = trainer.predict(model, colmod, return_predictions=True)\n",
    "\n",
    "    elif isinstance(model, BaseEstimator):\n",
    "      X_train, y_train = colmod.sklearn_setup(\"train\") \n",
    "      X_test, y_test = colmod.sklearn_setup(\"test\")\n",
    "\n",
    "      model.fit(X_train, y_train)\n",
    "      y_pred = model.predict(X_test).reshape(-1)\n",
    "\n",
    "    if torch.is_tensor(y_pred[0]): \n",
    "      y_pred = [elem.item() for tensor in y_pred for elem in tensor.flatten()]\n",
    "\n",
    "    actuals = []\n",
    "    for batch in colmod.predict_dataloader():\n",
    "      x, y = batch\n",
    "      actuals.extend(y.numpy())\n",
    "\n",
    "    actuals_flat = [item for sublist in actuals for item in sublist]\n",
    "  # plot overload visual (red on overload), and save to folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-11 00:00:00\n",
      "2023-01-03 23:00:00\n"
     ]
    }
   ],
   "source": [
    "print(colmod.val_dates[0])\n",
    "print(colmod.val_dates[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate overload\n",
    "threshold = 50\n",
    "baseload = np.mean(actuals_flat)*0.3\n",
    "actual_class = np.where(np.array(actuals_flat) + baseload > threshold, 1, 0)\n",
    "pred_class = np.where(np.array(y_pred) + baseload > threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(threshold)\n",
    "print(baseload)\n",
    "counts = Counter(actual_class)\n",
    "total = len(actual_class)\n",
    "percent_0 = (counts[0] / total) * 100\n",
    "percent_1 = (counts[1] / total) * 100\n",
    "\n",
    "print(f\"Percentage of 0: {percent_0:.2f}%\")\n",
    "print(f\"Percentage of 1: {percent_1:.2f}%\")\n",
    "counts = Counter(pred_class)\n",
    "total = len(pred_class)\n",
    "percent_0 = (counts[0] / total) * 100\n",
    "percent_1 = (counts[1] / total) * 100\n",
    "\n",
    "print(f\"Percentage of 0: {percent_0:.2f}%\")\n",
    "print(f\"Percentage of 1: {percent_1:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(actuals_flat, label='Actuals')\n",
    "plt.plot(y_pred, label=model_name, color='orange')\n",
    "plt.axhline(y=threshold, color='red', linestyle='--', label='Threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(actuals_flat) + baseload, label='Actuals')\n",
    "plt.plot(np.array(y_pred) + baseload, label=model_name, color='orange')\n",
    "plt.axhline(y=threshold, color='red', linestyle='--', label='Threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl#TP: when pred is 1 and actual is 1\n",
    "TP = np.sum((pred_class == 1) & (actual_class == 1))\n",
    "\n",
    "#TN: when pred is 0 and actual is 0\n",
    "TN = np.sum((pred_class == 0) & (actual_class == 0))\n",
    "\n",
    "#FP: when pred is 1 and actual is 0\n",
    "FP = np.sum((pred_class == 1) & (actual_class == 0))\n",
    "\n",
    "#FN: when pred is 0 and actual is 1\n",
    "FN = np.sum((pred_class == 0) & (actual_class == 1))\n",
    "\n",
    "print(TP, TN)\n",
    "print(FP, FN)\n",
    "\n",
    "metrics = []\n",
    "\n",
    "metrics.append({\n",
    "  'model': model_name,\n",
    "  'mae': mean_absolute_error(y_pred, actuals_flat),\n",
    "  'mse': mean_squared_error(y_pred, actuals_flat),\n",
    "  'acc': accuracy_score(TP, TN, FP, FN),\n",
    "  'pre': precision_score(TP, FP),\n",
    "  'rec': recall_score(TP, FN),\n",
    "})\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
